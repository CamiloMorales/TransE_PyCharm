{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TransE implementation:\n",
    "\n",
    "1. Loafing the data, creating the entity2id dictionaries and creating the sparse matrices for represeting each one of the files [\"train\", \"valid\", \"test\"].\n",
    "\n",
    "  1.Read the triples in the file and create the list of left entities, right entities and relations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import cPickle\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Put the freebase15k data absolute path here\n",
    "datapath = '../../FB15k/'\n",
    "assert datapath is not None\n",
    "\n",
    "if 'data' not in os.listdir('../'):\n",
    "    os.mkdir('../data')\n",
    "\n",
    "def parseline(line):\n",
    "    lhs, rel, rhs = line.split('\\t')\n",
    "    lhs = lhs.split(' ')\n",
    "    rhs = rhs.split(' ')\n",
    "    rel = rel.split(' ')\n",
    "    return lhs, rel, rhs\n",
    "\n",
    "#################################################\n",
    "### Creation of the entities/indices dictionnaries\n",
    "\n",
    "np.random.seed(753)\n",
    "\n",
    "entleftlist = []\n",
    "entrightlist = []\n",
    "rellist = []\n",
    "\n",
    "for datatyp in ['train']:\n",
    "    f = open(datapath + 'freebase_mtr100_mte100-%s.txt' % datatyp, 'r')\n",
    "    dat = f.readlines()\n",
    "    f.close()\n",
    "    for i in dat:\n",
    "        lhs, rel, rhs = parseline(i[:-1])\n",
    "        entleftlist += [lhs[0]]\n",
    "        entrightlist += [rhs[0]]\n",
    "        rellist += [rel[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * Since following the paper, the entity have the same embedding regardledd if it appeared on the lef or right part of the triple, we need to get rid of duplicates, in the same list as well as between left and right lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entleftset = np.sort(list(set(entleftlist) - set(entrightlist)))\n",
    "entsharedset = np.sort(list(set(entleftlist) & set(entrightlist)))\n",
    "entrightset = np.sort(list(set(entrightlist) - set(entleftlist)))\n",
    "relset = np.sort(list(set(rellist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * Creation of the dictionaries. We have 2 dictionaries: \n",
    "  * Entity2idx:keys are the entities and values are the ids.\n",
    "  * idx2entity: keys are ids and values are the entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity2idx = {}\n",
    "idx2entity = {}\n",
    "\n",
    "\n",
    "# we keep the entities specific to one side of the triplets contiguous\n",
    "idx = 0\n",
    "for i in entrightset:\n",
    "    entity2idx[i] = idx\n",
    "    idx2entity[idx] = i\n",
    "    idx += 1\n",
    "nbright = idx\n",
    "for i in entsharedset:\n",
    "    entity2idx[i] = idx\n",
    "    idx2entity[idx] = i\n",
    "    idx += 1\n",
    "nbshared = idx - nbright\n",
    "for i in entleftset:\n",
    "    entity2idx[i] = idx\n",
    "    idx2entity[idx] = i\n",
    "    idx += 1\n",
    "nbleft = idx - (nbshared + nbright)\n",
    "\n",
    "print \"# of only_left/shared/only_right entities: \", nbleft, '/', nbshared, '/', nbright\n",
    "# add relations at the end of the dictionary\n",
    "for i in relset:\n",
    "    entity2idx[i] = idx\n",
    "    idx2entity[idx] = i\n",
    "    idx += 1\n",
    "nbrel = idx - (nbright + nbshared + nbleft)\n",
    "print \"Number of relations: \", nbrel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * Finally we serialize the dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../data/FB15k_entity2idx.pkl', 'w')\n",
    "g = open('../data/FB15k_idx2entity.pkl', 'w')\n",
    "cPickle.dump(entity2idx, f, -1)\n",
    "cPickle.dump(idx2entity, g, -1)\n",
    "f.close()\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * We now proceed to the creation of the sparse matrices that will represent the dataset: 1 set of 3 matrices (left, right and rels) per file ([\"train\", \"valid\", \"test\"]). Each matrix will have the shape: total number of entities x total number of triples in the file. Each rowrepresents one entity, and whenever the row has a 1 means it appears on that triple. The conjunctions of the 3 sparse matrices (and the dictionaries) are the representation of the respective file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for datatyp in ['train', 'valid', 'test']:\n",
    "    print datatyp\n",
    "    f = open(datapath + 'freebase_mtr100_mte100-%s.txt' % datatyp, 'r')\n",
    "    dat = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "    # Declare the dataset variables\n",
    "    inpl = sp.lil_matrix((np.max(entity2idx.values()) + 1, len(dat)), dtype='float32')\n",
    "    inpr = sp.lil_matrix((np.max(entity2idx.values()) + 1, len(dat)), dtype='float32')\n",
    "    inpo = sp.lil_matrix((np.max(entity2idx.values()) + 1, len(dat)), dtype='float32')\n",
    "\n",
    "    # Fill the sparse matrices\n",
    "    ct = 0\n",
    "    for i in dat:\n",
    "        lhs, rel, rhs = parseline(i[:-1])\n",
    "        if lhs[0] in entity2idx and rhs[0] in entity2idx and rel[0] in entity2idx:\n",
    "            inpl[entity2idx[lhs[0]], ct] = 1\n",
    "            inpr[entity2idx[rhs[0]], ct] = 1\n",
    "            inpo[entity2idx[rel[0]], ct] = 1\n",
    "            ct += 1\n",
    "        else:\n",
    "            if lhs[0] in entity2idx:\n",
    "                unseen_ents+=[lhs[0]]\n",
    "            if rel[0] in entity2idx:\n",
    "                unseen_ents+=[rel[0]]\n",
    "            if rhs[0] in entity2idx:\n",
    "                unseen_ents+=[rhs[0]]\n",
    "            remove_tst_ex+=[i[:-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finally we serialize each matrix with its respective name regarding left, relation or right and [\"train\", \"valid\", \"test\"]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'data' not in os.listdir('../'):\n",
    "        os.mkdir('../data')\n",
    "    f = open('../data/FB15k-%s-lhs.pkl' % datatyp, 'w')\n",
    "    g = open('../data/FB15k-%s-rhs.pkl' % datatyp, 'w')\n",
    "    h = open('../data/FB15k-%s-rel.pkl' % datatyp, 'w')\n",
    "    cPickle.dump(inpl.tocsr(), f, -1)\n",
    "    cPickle.dump(inpr.tocsr(), g, -1)\n",
    "    cPickle.dump(inpo.tocsr(), h, -1)\n",
    "    f.close()\n",
    "    g.close()\n",
    "    h.close()\n",
    "\n",
    "unseen_ents=list(set(unseen_ents))\n",
    "print len(unseen_ents)\n",
    "remove_tst_ex=list(set(remove_tst_ex))\n",
    "print len(remove_tst_ex)\n",
    "\n",
    "for i in remove_tst_ex:\n",
    "    print i"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}